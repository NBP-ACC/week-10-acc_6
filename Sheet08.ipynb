{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osnabrück University - A&C: Computational Cognition (Summer Term 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 08: EEG Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in at 14:00 on **Tuesday, June 25, 2019**. If you need help (and Google and other resources were not enough), feel free to contact your tutors. Please push your results to your Github group folder.\n",
    "\n",
    "In this exercise sheet you will start to work with EEG data.\n",
    "\n",
    "The dataset is from an EEG study, conducted by Artur Czeszumski, Benedikt Ehinger, Basil Wahn and Peter König here at the institue. [Here](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00361/full) you can find their paper. I highly recommend to at least read the parts where the experimental design and the experimental procedure are described, in order to properly understand how the data was recorded. The whole data is openly distributed on the [Open Science Framework](https://osf.io/c4wkx/). \n",
    "Here's how you can access the data that we will work with:\n",
    "- Go to the section *Files* and select the *Data/EEG/sub408* \n",
    "- For all the homework regarding EEG data analysis we will be working with the data of *6_408_RerefInterp.set*. So this is the file you want to download. \n",
    "- Store it in the same folder as this notebook, in order to make it easily accessible.\n",
    "\n",
    "For this task, we will mainly use the [MNE](http://martinos.org/mne), which is an open-source python package for EEG data processing. Since *MNE* has many really nice functions, in this homework you will probably spend more time with reading and thinking than with actual coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For more interactive plot, you can uncomment the following code:\n",
    "%matplotlib qt5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 0: Peer review for sheet 06 [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, you will have to make a peer review of the other groups' solutions. Each group reviews the solutions of two other groups and give points according to the given point distribution considering the correctness of the solution. For this reviews the tutors will give you up to 3 points each week.\n",
    "\n",
    "| * |Group 1|Group 2|Group 3|Group 4|Group 5|Group 6|Group 7|Group 8|Group 9|Group 10|Group 11|\n",
    "| ------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------ | ------ |\n",
    "| check solutions of group: | 8, 3 | 1, 11  | 10, 8  | 5, 10 | 9, 2 | 11, 5  | 6, 4 | 2, 9  | 4, 6  | 7, 1   | 3, 7   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Looking at the the EEG data [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Loading the preprocessed EEG dataset [1 pt]\n",
    "The first step is to get an idea of the different data types, that are used by MNE. So once we loaded the EEG data, we will look at the *mne.info* attribute, which is like a modified Python dictionary. It stores all the metadata about the recordings. You can access all the keys by calling *raw.key*. The info-attribute tells you of what type the key-specific output will be.\n",
    "E.g. *preprocessed.ch_names* will return a list of strings containing all channel-names. \n",
    "\n",
    "- Print the metadata of the recorings\n",
    "- Print all channel-names as a list\n",
    "- Call preprocessed.plot() and limit the number of channels to be displayed at once to 10 in order to have a first look at the EEG recordings.\n",
    "\n",
    "*Info: Since the data has already been preprocesed (filtered, cleaned, etc.) with the EEGLAB toolbox in Matlab, not all the metadata is correctly transferred by MNE (e.g. the values for highpass and lowpass filters in the .info attribute are not correct, in the study they used a 0.1Hz HP filter and a 120Hz LP filter). For our purpose however, this does not matter. Yet, in general it would be recommendable to do both, preprocessing and analysis, with the same toolbox.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-b36976b70b7b>:11: DeprecationWarning: stim_channel (default True in 0.17) will change to False in 0.18 and be removed in 0.19, set it to False in 0.17 to avoid this warning\n",
      "  preprocessed = mne.io.read_raw_eeglab(fname, preload= True)\n",
      "<ipython-input-23-b36976b70b7b>:11: RuntimeWarning: Limited 157 annotation(s) that were expanding outside the data range.\n",
      "  preprocessed = mne.io.read_raw_eeglab(fname, preload= True)\n",
      "<ipython-input-23-b36976b70b7b>:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  preprocessed = mne.io.read_raw_eeglab(fname, preload= True)\n",
      "<ipython-input-23-b36976b70b7b>:11: RuntimeWarning: 199/2691 event codes could not be mapped to integers. Use the 'event_id' parameter to map such events manually.\n",
      "  preprocessed = mne.io.read_raw_eeglab(fname, preload= True)\n",
      "<ipython-input-23-b36976b70b7b>:11: RuntimeWarning: 1 events will be dropped because they occur on the same time sample as another event. `mne.io.Raw` objects store events on an event channel, which cannot represent two events on the same sample. Please use `read_annotations_eeglab` and create events using `events_from_annotations` to extract the original event structure. Then, you can e.g. subset the extracted events for constructing epochs.\n",
      "  preprocessed = mne.io.read_raw_eeglab(fname, preload= True)\n",
      "<ipython-input-23-b36976b70b7b>:11: RuntimeWarning: Events like the following will be dropped entirely: ['boundary', '__'], 2 in total\n",
      "  preprocessed = mne.io.read_raw_eeglab(fname, preload= True)\n"
     ]
    }
   ],
   "source": [
    " # We set the log-level to 'WARNING' so the output is less verbose\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "# create path to file where all EEG Data is stored\n",
    "home = os.getcwd()\n",
    "setpath = home + \"/Data/\"\n",
    "\n",
    "# load the preprocessed EEG data and display info-attribute\n",
    "setname = \"6_408_RerefInterp.set\"\n",
    "fname = setpath + setname\n",
    "preprocessed = mne.io.read_raw_eeglab(fname, preload= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the info attribute and see what it contains\n",
    "print(preprocessed.info)\n",
    "\n",
    "# print all channel-names\n",
    "print(preprocessed.ch_names)\n",
    "\n",
    "# plot all the data, only display 10 channels at once\n",
    "preprocessed.plot(n_channels=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get an idea about the location of the channels, you can run this cell. Here we read all the montages that are stored within MNE. A montage relates the channels' names to standardized or actual locations on the scalp surface. We will plot the standard *10-20 system*, since this is the one that was used for our recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-69629b880b0b>:8: RuntimeWarning: The following EEG sensors did not have a position specified in the selected montage: ['VEOG']. Their position has been left untouched.\n",
      "  preprocessed.set_montage(montage)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RawEEGLAB  |  6_408_RerefInterp.set, n_channels x n_times : 66 x 1546814 (3021.1 sec), ~779.0 MB, data loaded>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, list all montages that ship with mne:\n",
    "mne.channels.get_builtin_montages()\n",
    "# choose the 10-20 system and plot the a map of the scalp\n",
    "montage = mne.channels.read_montage(\"standard_1020\")\n",
    "montage.plot()\n",
    "\n",
    "#assign this montage to our preprocessed object\n",
    "preprocessed.set_montage(montage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Select and plot a data segment [2 pts]\n",
    "Now we will look at the preprocessed data, selecting a certain segment in time and some channels that we want plot.\n",
    "\n",
    "Generally the EEG data can be accessed as:  \n",
    "*data, times = preprocessed[picks, time_slice]*  \n",
    "... here *picks* selects the channels that you want to focus on, calling them by their index  \n",
    "\n",
    "You can get all the data simply by:  \n",
    "*data, times = preprocessed[ : ]*  \n",
    "\n",
    "Which is equivalent to:  \n",
    "*data, times = preprocessed[ :, :]*  \n",
    "\n",
    "With this information: \n",
    "- Write a function that plots a segment of the data, given a certain time-interval and a list of channels: Use it to plot a data segment between 100 and 115s and specifically select the midline frontal electrodes ['F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2'] with the *picks*-argument. In the paper mentioned above you can read more about why we focus on exactly these channels. \n",
    "- Print *times.shape* and *data.shape* before and after segmenting the data, in order to get an idea of their dimensions. The shapes of the two arrays (*data* and *times*) explain why we need to take the transpose of the data-array in order to be able to plot it.\n",
    "- First plot all the channels together by simply plotting *times* and *data*. \n",
    "- Then plot all channels separatly in subplots.\n",
    "\n",
    "**Hints:**  \n",
    "1. For *picks*, you will have to use *.ch_names.index('channel-name')* in order to get the index of the electrode-names that you want to select.  \n",
    "2. Also note that if you just slice your data with for example *data, times = preprocessed[:, :10]*, you are not selecting a time interval in seconds but only time steps or samples (here the first 10 time steps). This means, that you will first have to find the sample-index of your time-in-seconds interval. For this use *preprocessed.time_as_index([segment_in_seconds])*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Empty channel list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-aa0d6059c1fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mch_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mi2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mch_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FC2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m115\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/acc/lib/python3.6/site-packages/mne/io/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \"\"\"  # noqa: E501\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0msel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_get_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/acc/lib/python3.6/site-packages/mne/io/base.py\u001b[0m in \u001b[0;36m_parse_get_set_params\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty channel list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Empty channel list"
     ]
    }
   ],
   "source": [
    "# acces all data:\n",
    "channels = np.array(['F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2'])\n",
    "i1 = preprocessed.ch_names.index('F1')\n",
    "i2 = preprocessed.ch_names.index('FC2')\n",
    "data_all, times_all = preprocessed[i1:i2 , 100:115]\n",
    "print(data_all.shape)\n",
    "print(times_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that plots the segment of the data, given a certain time-interval and a list of channels\n",
    "\n",
    "def plotSegment(segment, channels):\n",
    "    # choose a data segment between 100s to 115s\n",
    "    start, stop = # TODO\n",
    "    # TODO\n",
    "    \n",
    "    # in order to check whether you sliced your data correctly, print:\n",
    "    # print(times.min(), times.max())\n",
    "    \n",
    "    return\n",
    "\n",
    "plotSegment([100,115], ch_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all channels separately\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Define and read epochs [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Find events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step we will epoch our data. This means that we extract specific time-windows from the continuous EEG signal. These time windows are called “epochs”. Usually they are time-locked with respect to an event, i.e. in our case the button presses of the subjects. We will use these epochs in the next step in order to compute our ERPs. Before we can do this, we first have to extract the event-information that is stored within our EEG-data with the function [*mne.find_events*](https://www.martinos.org/mne/stable/generated/mne.find_events.html). The resulting *events-object* is a NumPy array of shape (N, 3), where N is the number of events. The first column contains the event onsets (in samples), and the third column contains the event-codes.  \n",
    "- Find the events and print the first 30 entries in order to see how the array looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Create epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create our epochs. \n",
    "- As a first step, you have to create a dictionary that assigns all event numbers (as values) to the respective responses of the participants and the condition (as keys) that they belong to. For this, take the docx-file with the \"trigger\"-codes from your repository as a help. You may ignore all events, that do not refer to the participants' response. \n",
    "For the later steps it is important that your keys are named as following: *'Social_situation/Responses'*.    \n",
    "E.g. If both participants answered correctly in the cooporation conditon your key should be called: *'Coop/CC'*.  \n",
    "\n",
    "- Now, you can epoch your data with the help of this dictionary, using [mne.Epochs()](https://martinos.org/mne/dev/generated/mne.Epochs.html). One epoch should start 200ms before and end 500ms after the trigger/ the event. Define these parameters as *tmin* and *tmax*.\n",
    "- Once you created the epochs, you can get the data for all epochs of one condition with *epochs['condition'].get_data()*. \n",
    "- Check how many epochs have been created for each condition by just printing *epochs['social_situation']*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define dictionary with all event-codes\n",
    "event_id = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define epoch parameters\n",
    "tmin =   # TODO: start of each epoch (200ms before the trigger)\n",
    "tmax =   # TODO: end of each epoch (500ms after the trigger)\n",
    "\n",
    "# 2. Create Epochs\n",
    "epochs = #T ODO\n",
    "\n",
    "# 3. Check how many epochs have been created for each social condition (i.e. Cooporation, Competition):\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Compute  and plot Evoked responses for different conditions [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Compute ERPs [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compute our ERPs (averaged event-related cortical potentials) by averaging all epochs of one condition with *epochs['condition'].average()*.  \n",
    "- For this, create another dictionary: Here the ERPs should be stored as values, to which you assign the 'Win' or 'Loose' outcome of both social situations (i.e. cooporation, competition) as keys. In the end you should have 4 different conditions, two outcomes for each social situation (i.e. for cooporation you have 'Coop/Win' and 'Coop/Loose as dictionary-keys, an the respective ERP as dictionary-value). *Note that the outcome of 'Win' or 'Loose' should be assigned with respect to the particpant of which we measured the EEG data!*\n",
    "\n",
    "Refering to the [paper](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00361/full) of the study, you can fill the following table. This helps you find the event-code for the 'Win' and 'Loose' situation:\n",
    "\n",
    "| Social situation|Outcome\n",
    "| :-: |:-: |\n",
    "| **Cooperation** | WIN: both of the participants responded correctly | \n",
    "| -| LOOSE: **?** | \n",
    "| **Competition** | WIN:  One participant was correct and the other was incorrect | \n",
    "| - | (reward was added to the correct participant’s budget) |\n",
    "| - | LOOSE: **?** | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of ERPs with respect to WIN/ LOOSE events\n",
    "evoked_dict = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Visualize the ERPs [3 pts]\n",
    "\n",
    "Now we want to plot our ERPs.\n",
    "- As a first step, simply plot the ERPs of all channels, selecting the ERP of one of the conditions ('Coop/Win','Coop/Loose', etc.) from the from the just created dictionary in a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ERPs of all channels for a one of the conditions\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now use the same condition, but this time set the *picks*-argument to only plot the 6 midline frontal electrodes that we selected above. Note that you need a list of their indices, not just their names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ERPs for one of the conditions of selected channels\n",
    "pick = # TODO\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the ERPs from individual channels, it’s always useful to see topographical maps of our data. It’s a nice way to see what’s going on across the whole head, showing us whether effects are broadly or narrowly distributed across the whole scalp.\n",
    "- So now we want to plot the 2D topography of the evoked responses using [*mne.Evoked.plot_topomap()*](https://martinos.org/mne/stable/generated/mne.Evoked.html#mne.Evoked.plot_topomap). The first argument, *times* allows us to specify time instants (in seconds) for which topographies will be shown. Choose timepoints from 50 to 250ms with a step of 40ms with the help of *np.arrange()* and plot the topomaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = #TODO\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of showing topographies at specific time points, now compute averages of 50ms bins centered on these time points. As you might recognize that this slightly reduces the noise in the topographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to compare the ERPs of the different social situations and the different outcome, seeing whether the event related activity within the selected frontal electrodes differs between these conditions. For this use the function *mne.viz.plot_compare_evokeds()*. \n",
    "- Again use *picks* to select the 6 midline frontal electrodes.  \n",
    "- Apart from this, you also need to create two dictionaries: one assigning two different *colors* to the two possible outcomes (i.e. green and red for Win or Loose trials) and one assigning different *linestyles* to the different social situations (i.e. solid and dashed lines for either Cooporation or Competition).\n",
    "- Now plot the ERPs with the *plot_compare_evokeds* command. You should get a plot with 4 lines (dashed/red, dashed/green, solid/red, solid/green). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of frontal electrodes\n",
    "pick = # TODO (same as in 1b)\n",
    "# create dictionaries for color and linestyle\n",
    "colors = # TODO\n",
    "linestyles = # TODO\n",
    "\n",
    "# compare ERPs\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "[Czeszumski, A., Ehinger, B., Wahn, B., and Knig, P. (2019). *The social situation affects how we process feedback about our actions*. Frontiers in Psychology, 10:361.](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00361/full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
